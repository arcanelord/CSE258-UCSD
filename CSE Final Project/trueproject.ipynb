{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### first time to run this code starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88310"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(readGz('australian_users_items.json.gz'))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import data\n",
    "user = set()\n",
    "item = set()\n",
    "item_list= []\n",
    "user_list = []\n",
    "count = 0\n",
    "item_countlist = []\n",
    "\n",
    "\n",
    "for l in readGz('australian_users_items.json.gz'):\n",
    "    count += 1\n",
    "    user_id,items,item_count = l['user_id'],l['items'],l['items_count']\n",
    "    user.add(user_id)\n",
    "    item_countlist.append(item_count)\n",
    "    for i in items:\n",
    "        item.add(i['item_id'])\n",
    "        user_list.append(user_id)\n",
    "        item_list.append(i['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10978"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'user' : user_list, 'item' : item_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87626"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scraping for all gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scraping\n",
    "\n",
    "# '''import time\n",
    "# from random import choice #This library helps pick a random item from a list\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import urllib\n",
    "\n",
    "# user_agents = [\n",
    "#     'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "#     'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "#     'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "# ]\n",
    "\n",
    "# def random_headers():\n",
    "#     return {'User-Agent': choice(user_agents),'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}'''\n",
    "\n",
    "# '''# Parse all the gen type from steam\n",
    "# r = requests.get('https://store.steampowered.com/tag/browse/#global_492',headers = random_headers())\n",
    "# html = r.text\n",
    "# soup = BeautifulSoup(html, 'lxml')\n",
    "# gen = []\n",
    "# if soup.find('div',{'class': 'class=\"tag_browse_tags\"'}) != None:\n",
    "#     for i in soup.find('div',{'class': 'class=\"tag_browse_tags\"'}).find_all('data-tagid'):\n",
    "#         gen.append(i.get_text())'''\n",
    "\n",
    "# '''# build a gen type list for later use\n",
    "# gen_type = []\n",
    "# for i in soup.find('div', id = 'tag_browse_global'):\n",
    "#     gen_type.append(i.string)\n",
    "# gen_type = [gen_type[i] for i in range(1,711,2)]'''\n",
    "\n",
    "# '''gen_type = pd.DataFrame({'type' : gen_type})'''\n",
    "\n",
    "# '''# Save gen_type to csv\n",
    "# gen_type.to_csv('gen_type.csv', index = False, sep = \",\")'''\n",
    "\n",
    "# '''import string\n",
    "# item_id_gen= defaultdict(list)\n",
    "\n",
    "# for l in readGz('bundle_data.json.gz'):\n",
    "#     for w in l['items']:\n",
    "#         for i in gen_type['type']:\n",
    "#             if i in w['genre']:\n",
    "#                 item_id_gen[w['item_id']].append(i)'''\n",
    "\n",
    "# '''Ori_gen_type = set()\n",
    "# for l in item_id_gen.values():\n",
    "#     for i in l:\n",
    "#         Ori_gen_type.add(i)\n",
    "# Ori_gen_type'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bitem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dfb1a57a5e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreadGz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bundle_data.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mbitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bitem' is not defined"
     ]
    }
   ],
   "source": [
    "for l in readGz('bundle_data.json.gz'):\n",
    "    for w in l['items']:\n",
    "        bitem.add(w['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitem = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no rerun get from csv\n",
    "table = table.sample(frac=1).reset_index(drop=True)\n",
    "Train = table[:2576605]\n",
    "Validation = table[2576605:3864907]\n",
    "Test = table[3864907:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun\n",
    "T_userlist = Train['user']\n",
    "T_itemlist = Train['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no rerun get from csv\n",
    "import datetime as dt\n",
    "time1 = dt.datetime.now()\n",
    "user_item_id = defaultdict(list)\n",
    "for i in range(len(Train)):\n",
    "    user_item_id[Train.at[i,'user']].append(Train.at[i,'item'])\n",
    "time2 = dt.datetime.now()\n",
    "print(\"It takes \",str(time2 - time1),\" to run this\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_id_whole={}\n",
    "for l in readGz('australian_users_items.json.gz'):\n",
    "    user_id = l['user_id']\n",
    "    itemtemp=[]\n",
    "    user_item_id_whole[user_id]=[]\n",
    "    for w in l['items']:\n",
    "        itemtemp.append(w['item_id'])\n",
    "        user_item_id_whole[user_id]=itemtemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no rerun\n",
    "NVal_user = []\n",
    "NVal_item = []\n",
    "time1 = dt.datetime.now()\n",
    "for u in list(user):\n",
    "    NVal_user += [u]*40\n",
    "    NVal_item += random.sample([i for i in list(item) if i not in user_item_id_whole[u]],40)\n",
    "    if len(NVal_user) > len(Train):\n",
    "        break\n",
    "Vali_len = len(Validation)\n",
    "Test_len = len(Test)\n",
    "NPUI = pd.DataFrame({'user' : NVal_user, 'item' : NVal_item})\n",
    "NPUI = NPUI.sample(frac=1).reset_index(drop=True)\n",
    "Vali_np = NPUI[:Vali_len]\n",
    "Test_np = NPUI[Vali_len:]\n",
    "valilabel = [1]*len(Validation) + [0] * len(Vali_np)\n",
    "testlabel = [1]*len(Test) + [0] * len(Test_np)\n",
    "Validationdf = pd.concat([Validation, Vali_np])\n",
    "Validationdf['label'] = valilabel\n",
    "Testdf = pd.concat([Test, Test_np])\n",
    "Testdf['label'] = testlabel\n",
    "Validationdf.to_csv(\"Validationset.csv\",index = False, sep=\",\")\n",
    "Testdf.to_csv(\"Testset.csv\",index = False, sep=\",\")\n",
    "Train.to_csv(\"Trainset.csv\",index = False, sep=\",\")\n",
    "time2 = dt.datetime.now()\n",
    "print(\"It takes \",str(time2 - time1),\" to run this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_item = [i for i in item if i not in item_id_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### web scarping for item_id_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''no_item = [i for i in item if i not in item_id_gen]'''\n",
    "\n",
    "# '''time1 = dt.datetime.now()\n",
    "# r = requests.get(\"https://store.steampowered.com/app/\"+str(242590),headers = random_headers())\n",
    "# html = r.content\n",
    "# soup = BeautifulSoup(html, 'html5lib')\n",
    "# gen = []\n",
    "# if soup.find('div',{'class': 'details_block'}) != None:\n",
    "#     gen = [i.get_text() for i in soup.find('div',{'class': 'details_block'}).find_all(href = re.compile(\"^https://store.steampowered.com/genre\"))]\n",
    "# print(gen)\n",
    "# print(dt.datetime.now()-time1)'''\n",
    "\n",
    "# '''# parse gen data from steam\n",
    "# time1 = dt.datetime.now()\n",
    "# count = 0\n",
    "\n",
    "# for d in no_item:\n",
    "#     try:\n",
    "#         r = requests.get(\"https://store.steampowered.com/app/\"+str(d),headers = random_headers())\n",
    "#         r.encoding = 'utf-8'\n",
    "#         html = r.text\n",
    "#         soup = BeautifulSoup(html, 'lxml')\n",
    "#         gen = []\n",
    "#         if soup.find('div',{'class': 'details_block'}) != None:\n",
    "#             gen = [i.string for i in soup.find('div',{'class': 'details_block'}).find_all(href = re.compile(\"^https://store.steampowered.com/genre\"))]\n",
    "#         item_id_gen[d] = gen\n",
    "#         count += 1\n",
    "#         if count % 10 == 0:\n",
    "#             time.sleep(random.uniform(3,4))\n",
    "#         elif count % 5 == 0:\n",
    "#             time.sleep(random.uniform(1,2))\n",
    "#     except urllib.error.URLError:\n",
    "#         print(d,\"has error\")\n",
    "#         pass\n",
    "#     if count % 96 == 0:\n",
    "#         print(\"already done %.1f\"%(count*100/len(no_item)),\"% of data recovery, using\",(dt.datetime.now()-time1))\n",
    "\n",
    "# print(count,\"successful try, it takes:\",(dt.datetime.now()-time1),\"to crawl them in total. End of crawling\")'''\n",
    "\n",
    "# '''it = []\n",
    "# ge = []\n",
    "# for i in item_id_gen:\n",
    "#     it.append(i)\n",
    "#     ge.append(item_id_gen[i])\n",
    "\n",
    "# IG = pd.DataFrame({\"item\":it,\"gen\":ge})\n",
    "# IG.to_csv(\"IGdict5.csv\",index = False, sep = \",\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second time to run this code starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 356 lines\n",
      "process 12383 lines\n",
      "It takes  0:01:53.253865  to generate user_item_id\n",
      "It takes 0:00:01.094336 to generate user_gen_org\n",
      "It takes 0:00:02.856979 to generate user_gen\n",
      "It takes 0:01:27.329314 to generate item_user_id\n",
      "It takes 0:03:34.742834 to run all the things in part 2\n"
     ]
    }
   ],
   "source": [
    "# for the second time\n",
    "\n",
    "\n",
    "\n",
    "# import packages\n",
    "\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import sample\n",
    "from random import sample\n",
    "import csv\n",
    "import math\n",
    "time0 = dt.datetime.now()\n",
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)\n",
    "\n",
    "# generate validation set and train set\n",
    "\n",
    "with open('Validationset.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    Validationdf=pd.DataFrame([l for l in csv_reader])\n",
    "\n",
    "with open('Trainset.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    Traindf=pd.DataFrame([l for l in csv_reader])\n",
    "\n",
    "def redf(dataframe):\n",
    "    dataframe = dataframe.drop([0])\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    if len(dataframe.columns) == 3:\n",
    "        dataframe.columns = ['user','item','label']\n",
    "    if len(dataframe.columns) == 2:\n",
    "        dataframe.columns = ['user','item']\n",
    "    return(dataframe)\n",
    "\n",
    "Validationdf = redf(Validationdf)\n",
    "Traindf = redf(Traindf)\n",
    "\n",
    "# generate all genre\n",
    "\n",
    "gen_type = []\n",
    "with open('gen_type.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    for row in csv_reader:\n",
    "        gen_type.append(row[0])\n",
    "        line += 1\n",
    "    print(\"process %i lines\" %line)\n",
    "\n",
    "\n",
    "gen_type = gen_type[1:]\n",
    "\n",
    "# generate dict item_id_gen\n",
    "\n",
    "item_id_gen = defaultdict(list)\n",
    "with open('IGdict5.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    for row in csv_reader:\n",
    "        item_id_gen[row[0]] = []\n",
    "        for i in gen_type:\n",
    "            if i in row[1]:\n",
    "                item_id_gen[row[0]].append(i)\n",
    "        line += 1\n",
    "    print(\"process %i lines\" %line)\n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "w = Traindf['user']\n",
    "y = Traindf['item']\n",
    "user_item_id = defaultdict(list)\n",
    "for i in range(len(w)):\n",
    "    user_item_id[w[i]].append(y[i])\n",
    "time2 = dt.datetime.now()\n",
    "print(\"It takes \",str((time2 - time1)),\" to generate user_item_id\")\n",
    "\n",
    "w = set(Traindf['user'])\n",
    "time1 = dt.datetime.now()\n",
    "user_gen_org = {}\n",
    "for u in w:\n",
    "    items = []\n",
    "    for i in user_item_id[u]:\n",
    "        items += item_id_gen[i]\n",
    "    user_gen_org[u] = items\n",
    "time2 = dt.datetime.now()\n",
    "print(\"It takes \"+str((time2 - time1))+\" to generate user_gen_org\") \n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "user_gen = {}\n",
    "for u in user_gen_org:\n",
    "    user_gen[u] = list(set(user_gen_org[u]))\n",
    "time2 = dt.datetime.now()\n",
    "print(\"It takes \"+str((time2 - time1))+\" to generate user_gen\")\n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "item_user_id = defaultdict(list)\n",
    "for i in range(len(Traindf)):\n",
    "    item_user_id[Traindf.at[i,'item']].append(Traindf.at[i,'user'])\n",
    "time2 = dt.datetime.now()\n",
    "print(\"It takes \"+str((time2 - time1))+\" to generate item_user_id\")\n",
    "\n",
    "time3 = dt.datetime.now()\n",
    "\n",
    "print(\"It takes \"+str((time3 - time0))+\" to run all the things in part 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 12383 lines\n"
     ]
    }
   ],
   "source": [
    "item_id_gen = defaultdict(list)\n",
    "with open('IGdict5.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    for row in csv_reader:\n",
    "        item_id_gen[row[0]] = []\n",
    "        for i in gen_type:\n",
    "            if i in row[1]:\n",
    "                item_id_gen[row[0]].append(i)\n",
    "        line += 1\n",
    "    print(\"process %i lines\" %line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12383"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(item_id_gen.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline model based on total portion of purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738130500457191"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From here is the baseline model\n",
    "\n",
    "# generate list of 50% top popular items\n",
    "\n",
    "totalPurchases = 0\n",
    "\n",
    "itemcount={}\n",
    "for i in user_item_id:\n",
    "    for w in user_item_id[i]:\n",
    "        item = w\n",
    "        if item not in itemcount:\n",
    "            itemcount[item] = 1\n",
    "        else:\n",
    "            itemcount[item] += 1\n",
    "        totalPurchases += 1\n",
    "        \n",
    "mostPopular = [(itemcount[x], x) for x in itemcount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPurchases*0.5: break\n",
    "        \n",
    "# generate prediction of baseline model\n",
    "\n",
    "Validation_user = Validationdf['user']\n",
    "Validation_item = Validationdf['item']\n",
    "\n",
    "purchase = []\n",
    "for l in range(len(Validation_user)):\n",
    "    u,i =Validation_user[l],Validation_item[l]\n",
    "    if i in return1:\n",
    "        purchase.append('1')\n",
    "    else:\n",
    "        purchase.append('0')\n",
    "        \n",
    "# accuracy of the baseline\n",
    "\n",
    "sum(purchase == Validationdf['label'])/len(purchase)\n",
    "    \n",
    "# prediction_baseline = pd.DataFrame()\n",
    "# prediction_baseline['user']=Validation_user\n",
    "# prediction_baseline['item']=Validation_item\n",
    "# prediction_baseline['label']=purchase\n",
    "# prediction_baseline.to_csv(\"prediction_baseline.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defination of jaccard\n",
    "\n",
    "def jaccard_c(cate1,cate2):\n",
    "    try:\n",
    "        count = 0\n",
    "        for cate in cate1:\n",
    "            if cate in cate2:\n",
    "                count += 1\n",
    "        uni = len(cate1) + len(cate2) - count\n",
    "        jac = count / uni\n",
    "    except:\n",
    "        jac = 0\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jaccard based on genre of user and item (no repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:58.411638\n"
     ]
    }
   ],
   "source": [
    "time1 = dt.datetime.now()\n",
    "jaccard_clist = []\n",
    "w = Validationdf['user']\n",
    "y = Validationdf['item']\n",
    "for i in range(len(w)):\n",
    "    try:\n",
    "        usercat = user_gen[w[i]]\n",
    "        itemcat = item_id_gen[y[i]]\n",
    "    except:\n",
    "        usercat = []\n",
    "        itemcat = []\n",
    "    jaccard_clist.append(jaccard_c(usercat,itemcat))\n",
    "time2 = dt.datetime.now()\n",
    "print(time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 2576604, index implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7db48703241d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpurchase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaccard_clist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpurchase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpurchase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             return self._constructor(res_values, index=self.index,\n\u001b[0;32m-> 1291\u001b[0;31m                                      name=res_name, dtype='bool')\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0;34m'Length of passed values is {val}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                             \u001b[0;34m'index implies {ind}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                             .format(val=len(data), ind=len(index)))\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of passed values is 2576604, index implies 1"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "www = []\n",
    "for i in Validationdf['label']:\n",
    "    www.append(int(i))\n",
    "    www = pd.Series(www)\n",
    "    time1 = dt.datetime.now()\n",
    "    for i in np.arange(0, 1, 0.01):\n",
    "        purchase = list(np.array(jaccard_clist) > i)\n",
    "        acc.append(sum(purchase == www)/len(purchase))\n",
    "time2 = dt.datetime.now()\n",
    "print((time2 - time1))\n",
    "print('best i is ' + str(np.arange(0, 2, 0.1)[acc.index(max(acc))])+' and best accracy is '+str(max(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jaccard based on items of user and user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the time to run prediction is really long, I randomly take 1000 random data to test the accracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 0.01% random index\n",
    "\n",
    "indexorg = np.arange(0,len(Traindf))\n",
    "indexorg = list(indexorg)\n",
    "indi = sample(indexorg, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate jaccard list for the 1000 randoms\n",
    "\n",
    "x = Validationdf['user']\n",
    "y = Validationdf['item']\n",
    "jaccard_clist = []\n",
    "count = 0\n",
    "time1 = dt.datetime.now()\n",
    "for i in indi:\n",
    "    user = x[i]\n",
    "    item = y[i]\n",
    "    jacsum = 0\n",
    "    count += 1\n",
    "    try:\n",
    "        cat1 = user_item_id[user]\n",
    "        userlist = item_user_id[item]\n",
    "        for j in userlist:\n",
    "            cat2 = user_item_id[j]\n",
    "            jacsum += jaccard_c(cat1,cat2)\n",
    "        jacsum = jacsum/len(userlist)\n",
    "        jaccard_clist.append(jacsum)\n",
    "    except:\n",
    "        jaccard_clist.append(jacsum)\n",
    "    time2 = dt.datetime.now()\n",
    "print('it takes',str(time2-time1),'to generate the jaccard list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truepurchase = []\n",
    "label = Validationdf['label']\n",
    "acc = []\n",
    "\n",
    "for i in indi:\n",
    "    truepurchase.append(int(label[i]))\n",
    "truepurchase = pd.Series(truepurchase)\n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "for i in np.arange(0, 0.5, 0.001):\n",
    "    purchase = list(np.array(jaccard_clist) > i)\n",
    "    acc.append(sum(purchase == truepurchase)/len(purchase))\n",
    "time2 = dt.datetime.now()\n",
    "\n",
    "print('it takes',str(time2 - time1),'to generate the prediction accracy')\n",
    "print('best threshold is ' + str(np.arange(0, 0.5, 0.001)[acc.index(max(acc))])+' and best accracy is '+str(max(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity part based on item purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_item_fast(user1,user2):\n",
    "    if user1 in user_item_id and user2 in user_item_id:\n",
    "        itemdict1 = {i:0 for i in set(user_item_id[user1]).union(set(user_item_id[user2]))}\n",
    "        itemdict2 = {i:0 for i in set(user_item_id[user1]).union(set(user_item_id[user2]))} \n",
    "        for i in user_item_id[user1]:\n",
    "            if i in itemdict1:\n",
    "                itemdict1[i]+=1\n",
    "        for i in user_item_id[user2]:\n",
    "            if i in itemdict2:\n",
    "                itemdict2[i]+=1\n",
    "        return(list(itemdict1.values()),list(itemdict2.values()))\n",
    "    else:\n",
    "        return(0,0)\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "def item_sim_fast(gitem,guser):\n",
    "    T_cs = 0\n",
    "    l=1\n",
    "    if gitem in item_user_id:\n",
    "        itembuyer = item_user_id[gitem]\n",
    "        if len(itembuyer) > 30:\n",
    "            for b in random.sample(itembuyer,30):\n",
    "                encode1,encode2 = encode_item_fast(guser,b)\n",
    "                T_cs += cos_sim(encode1,encode2)\n",
    "            l = 30\n",
    "        elif len(itembuyer) != 0:\n",
    "            for b in itembuyer:\n",
    "                encode1,encode2 = encode_item_fast(guser,b)\n",
    "                T_cs += cos_sim(encode1,encode2)\n",
    "            l = len(itembuyer)\n",
    "        else:\n",
    "            l = 1\n",
    "    else:\n",
    "        itembuyer = [0]\n",
    "    return(T_cs/l)\n",
    "\n",
    "def accuracy(prediction,label):\n",
    "    TRUE = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == label[i]:\n",
    "            TRUE += 1\n",
    "    return(TRUE/len(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cos_similarity and find optimal threshold to decide if buy or not  (Validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0.000000 % of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "Vali_index = random.sample(range(1,len(Validationdf)),10000)\n",
    "Vali_exam = Validationdf.loc[Vali_index,]\n",
    "Vali_exam = Vali_exam.reset_index(drop = True)\n",
    "CS = []\n",
    "for i in range(len(Vali_exam)):\n",
    "    CS.append(item_sim_fast(Vali_exam.at[i,'item'],Vali_exam.at[i,'user']))\n",
    "    if i % 1000 == 0:\n",
    "        print('Process %f %% of data' %(i*100/len(Vali_exam)))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using threshold of 0.071636, we get accuracy of 0.816800\n"
     ]
    }
   ],
   "source": [
    "label = Vali_exam['label']\n",
    "\n",
    "Observe = {}\n",
    "for threshold in np.linspace(0.068,0.072,100):\n",
    "    prediction = []\n",
    "    for cs in CS:\n",
    "        if cs>threshold:\n",
    "            prediction.append('1')\n",
    "        else:\n",
    "            prediction.append('0')\n",
    "    Observe[threshold] = accuracy(prediction,label)\n",
    "optimal = max(Observe.values())\n",
    "optimal_thresh = list(Observe.keys())[list(Observe.values()).index(optimal)]\n",
    "print(\"When using threshold of %f, we get accuracy of %f\" %(optimal_thresh,optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use optimal threshold to predict testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust threshold\n",
    "with open('Testset.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    Testdf=pd.DataFrame([l for l in csv_reader])\n",
    "    \n",
    "Testdf = redf(Testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0.000000 % of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "When using threshold of 0.069455, we get accuracy of 0.815800\n"
     ]
    }
   ],
   "source": [
    "Test_index = random.sample(range(1,len(Testdf)),10000)\n",
    "Test_exam = Testdf.loc[Test_index,]\n",
    "Test_exam = Test_exam.reset_index(drop = True)\n",
    "CS = []\n",
    "for i in range(len(Test_exam)):\n",
    "    CS.append(item_sim_fast(Test_exam.at[i,'item'],Test_exam.at[i,'user']))\n",
    "    if i % 1000 == 0:\n",
    "        print('Process %f %% of data' %(i*100/len(Test_exam)))\n",
    "\n",
    "# change threshold\n",
    "threshold = 0.069455\n",
    "label = Test_exam['label']\n",
    "prediction = []\n",
    "for cs in CS:\n",
    "    if cs>threshold:\n",
    "        prediction.append('1')\n",
    "    else:\n",
    "        prediction.append('0')\n",
    "print(\"When using threshold of %f, we get accuracy of %f\" %(threshold,accuracy(prediction,label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity part based on gen type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_gen_fast(user1,user2):\n",
    "    if user1 in user_gen_org and user2 in user_gen_org:\n",
    "        gendict1 = {i:0 for i in set(user_gen_org[user1]).union(set(user_gen_org[user2]))}\n",
    "        gendict2 = {i:0 for i in set(user_gen_org[user1]).union(set(user_gen_org[user2]))}\n",
    "        for g in user_gen_org[user1]:\n",
    "            if g in gendict1:\n",
    "                gendict1[g]+=1\n",
    "        for g in user_gen_org[user2]:\n",
    "            if g in gendict2:\n",
    "                gendict2[g]+=1\n",
    "        return(list(gendict1.values()),list(gendict2.values()))\n",
    "    else:\n",
    "        return(0,0)\n",
    "\n",
    "\n",
    "def gen_sim_fast(gitem,guser):\n",
    "    T_cs = 0\n",
    "    l=1\n",
    "    if gitem in item_id_gen:\n",
    "        itembuyer = item_user_id[gitem]\n",
    "        if len(itembuyer) > 30:\n",
    "            for b in random.sample(itembuyer,30):\n",
    "                encode1,encode2 = encode_gen_fast(guser,b)\n",
    "                T_cs += cos_sim(encode1,encode2)\n",
    "            l = 30\n",
    "        elif len(itembuyer) != 0:\n",
    "            for b in itembuyer:\n",
    "                encode1,encode2 = encode_gen_fast(guser,b)\n",
    "                T_cs += cos_sim(encode1,encode2)\n",
    "            l = len(itembuyer)\n",
    "    else:\n",
    "        itembuyer = [0]\n",
    "    return(T_cs/l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vali_index = random.sample(range(1,len(Validationdf)),10000)\n",
    "Vali_exam = Validationdf.loc[Vali_index,]\n",
    "Vali_exam = Vali_exam.reset_index(drop = True)\n",
    "CS = []\n",
    "for i in range(len(Vali_exam)):\n",
    "    CS.append(gen_sim_fast(Vali_exam.at[i,'item'],Vali_exam.at[i,'user']))\n",
    "    if i % 1000 == 0:\n",
    "        print('Process %f %% data' %(i*100/len(Vali_exam)))\n",
    "label = Vali_exam['label']\n",
    "\n",
    "Observe = {}\n",
    "for threshold in np.linspace(0.0001,0.001,100):\n",
    "    prediction = []\n",
    "    for cs in CS:\n",
    "        if cs>threshold:\n",
    "            prediction.append('1')\n",
    "        else:\n",
    "            prediction.append('0')\n",
    "    Observe[threshold] = accuracy(prediction,label)\n",
    "optimal = max(Observe.values())\n",
    "optimal_thresh = list(Observe.keys())[list(Observe.values()).index(optimal)]\n",
    "print(\"When using threshold of %f, we get accuracy of %f\" %(optimal_thresh,optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIve a plus score on hot game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot game\n",
    "\n",
    "### www here should be replaced by trainset's diction which is user to item\n",
    "totalPurchases = 0\n",
    "itemcount=defaultdict(int)\n",
    "for i in user_item_id:\n",
    "    for w in user_item_id[i]:\n",
    "        item = w\n",
    "        itemcount[item] += 1\n",
    "        totalPurchases += 1\n",
    "        \n",
    "mostPopular = [(itemcount[x], x) for x in itemcount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPurchases*0.5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Action': 13,\n",
       "             'Indie': 5,\n",
       "             'Simulation': 2,\n",
       "             'Adventure': 3,\n",
       "             'Casual': 1,\n",
       "             'Free to Play': 4,\n",
       "             'RPG': 4,\n",
       "             'Massively Multiplayer': 3,\n",
       "             'Multiplayer': 3})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top_20_gen = defaultdict(int)\n",
    "for ic, i in mostPopular[:20]:\n",
    "    for g in item_id_gen[i]:\n",
    "        Top_20_gen[g] += 1\n",
    "Top_20_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vali_index = random.sample(range(1,len(Validationdf)),1000)\n",
    "Vali_exam = Validationdf.loc[Vali_index,]\n",
    "Vali_exam = Vali_exam.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in Validationset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0.000000 % of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092100 score for hot game, when using threshold of 0.080000, we get accuracy of 0.876000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092122 score for hot game, when using threshold of 0.082778, we get accuracy of 0.867000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092144 score for hot game, when using threshold of 0.080000, we get accuracy of 0.877000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092167 score for hot game, when using threshold of 0.081111, we get accuracy of 0.873000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092189 score for hot game, when using threshold of 0.080000, we get accuracy of 0.873000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092211 score for hot game, when using threshold of 0.081667, we get accuracy of 0.871000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092233 score for hot game, when using threshold of 0.081667, we get accuracy of 0.874000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092256 score for hot game, when using threshold of 0.080000, we get accuracy of 0.869000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092278 score for hot game, when using threshold of 0.083889, we get accuracy of 0.871000\n",
      "Process 0.000000 % of data\n",
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "done\n",
      "Plusing 0.092300 score for hot game, when using threshold of 0.081667, we get accuracy of 0.874000\n"
     ]
    }
   ],
   "source": [
    "for plus in np.linspace(0.0921,0.0923,10):\n",
    "    CS = []\n",
    "    for i in range(len(Vali_exam)):\n",
    "        if Vali_exam.at[i,'item'] in return1:\n",
    "            CS.append(item_sim_fast(Vali_exam.at[i,'item'],Vali_exam.at[i,'user'])+plus)\n",
    "        else:\n",
    "            CS.append(item_sim_fast(Vali_exam.at[i,'item'],Vali_exam.at[i,'user']))\n",
    "        if i % int(len(Vali_exam)/10) == 0:\n",
    "            print('Process %f %% of data' %(i*100/len(Vali_exam)))\n",
    "    print('done')\n",
    "\n",
    "    label = Vali_exam['label']\n",
    "    Observe = {}\n",
    "    for threshold in np.linspace(0.08,0.085,10):\n",
    "        prediction = []\n",
    "        for cs in CS:\n",
    "            if cs>threshold:\n",
    "                prediction.append('1')\n",
    "            else:\n",
    "                prediction.append('0')\n",
    "        Observe[threshold] = accuracy(prediction,label)\n",
    "    optimal = max(Observe.values())\n",
    "    optimal_thresh = list(Observe.keys())[list(Observe.values()).index(optimal)]\n",
    "    print(\"Plusing %f score for hot game, when using threshold of %f, we get accuracy of %f\" %(plus, optimal_thresh,optimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0.000000 % of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 10.000000 % of data\n",
      "Process 20.000000 % of data\n",
      "Process 30.000000 % of data\n",
      "Process 40.000000 % of data\n",
      "Process 50.000000 % of data\n",
      "Process 60.000000 % of data\n",
      "Process 70.000000 % of data\n",
      "Process 80.000000 % of data\n",
      "Process 90.000000 % of data\n",
      "When using threshold of 0.085000, we get accuracy of 0.873100\n"
     ]
    }
   ],
   "source": [
    "Test_index = random.sample(range(1,len(Testdf)),10000)\n",
    "Test_exam = Testdf.loc[Test_index,]\n",
    "Test_exam = Test_exam.reset_index(drop = True)\n",
    "plus = 0.092144\n",
    "CS = []\n",
    "for i in range(len(Test_exam)):\n",
    "    if Test_exam.at[i,'item'] in return1:\n",
    "        CS.append(item_sim_fast(Test_exam.at[i,'item'],Test_exam.at[i,'user'])+plus)\n",
    "    else:\n",
    "        CS.append(item_sim_fast(Test_exam.at[i,'item'],Test_exam.at[i,'user']))\n",
    "    if i % 1000 == 0:\n",
    "        print('Process %f %% of data' %(i*100/len(Test_exam)))\n",
    "\n",
    "# change threshold\n",
    "threshold = 0.085000\n",
    "label = Test_exam['label']\n",
    "prediction = []\n",
    "for cs in CS:\n",
    "    if cs>threshold:\n",
    "        prediction.append('1')\n",
    "    else:\n",
    "        prediction.append('0')\n",
    "print(\"When using threshold of %f, we get accuracy of %f\" %(threshold,accuracy(prediction,label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall =0.885009030704395, precision =0.863689776733255\n"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == label[i]:\n",
    "        if prediction[i] == '1':\n",
    "            TP += 1\n",
    "        elif prediction[i] == '0':\n",
    "            TN += 1\n",
    "    else:\n",
    "        if prediction[i] == '1':\n",
    "            FP += 1\n",
    "        elif prediction[i] == '0':\n",
    "            FN += 1\n",
    "\n",
    "recall = TP/(TP + FN)\n",
    "precision = TP/(TP + FP)\n",
    "print('recall =' + str(recall) + ', precision =' + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "label\n",
    "prediction\n",
    "CS\n",
    "index_l_yes = [i for i in range(len(label)) if label[i] == '1']\n",
    "index_l_no = [i for i in range(len(label)) if label[i] == '0']\n",
    "box1 = [CS[n] for n in index_l_no]\n",
    "box2 = [CS[y] for y in index_l_yes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"0\":box1, \"1\":box2[:len(box1)]})\n",
    "boxplot = df.boxplot(column=['0', '1'],figsize=(5,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[798]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(box2)) if box2[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box2.pop(798)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
