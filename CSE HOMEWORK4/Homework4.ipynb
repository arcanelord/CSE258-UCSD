{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "### Just the first 5000 reviews\n",
    "\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse158/data/beer/beer_50000.json\"))[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for l in data:\n",
    "    r = ''.join([c for c in l['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4587, 'with a'),\n",
       " (2595, 'in the'),\n",
       " (2245, 'of the'),\n",
       " (2058, 'is a'),\n",
       " (2033, 'on the')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [(cobCount[w], w) for w in cobCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "counts[0:5]\n",
    "#Question1  top 5 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qusetion2 \n",
    "\n",
    "pop_bigrams = [x[1] for x in counts[:1000]]\n",
    "bigramsId = dict(zip(pop_bigrams, range(len(pop_bigrams))))\n",
    "bigramsSet = set(pop_bigrams)\n",
    "def feature(datum):\n",
    "    old_w = ''\n",
    "    cob=''\n",
    "    feat = [0]*len(pop_bigrams)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "        if cob in pop_bigrams:\n",
    "            feat[bigramsId[cob]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3430153408719768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= 0\n",
    "for i in range(len(y)):\n",
    "    s +=(y[i]-predictions[i])**2\n",
    "MSE = s/len(y)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "review_list = []\n",
    "for datum in data:\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    temporary=''\n",
    "    for w in r.split():\n",
    "        temporary += w +' '\n",
    "    review_list.append(temporary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(review_list)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foam 1.1378686206869628\n",
      "smell 0.5379016188648442\n",
      "banana 1.6777807052660807\n",
      "lactic 2.9208187539523753\n",
      "tart 1.8068754016455384\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "count_foam= 0\n",
    "for i in range(len(review_list)):\n",
    "    if 'foam' in review_list[i].split():\n",
    "        count_foam+= 1\n",
    "count_smell= 0\n",
    "for i in range(len(review_list)):\n",
    "    if 'smell' in review_list[i].split():\n",
    "        count_smell+= 1\n",
    "count_banana= 0\n",
    "for i in range(len(review_list)):\n",
    "    if 'banana' in review_list[i].split():\n",
    "        count_banana+= 1\n",
    "count_lactic= 0\n",
    "for i in range(len(review_list)):\n",
    "    if 'lactic' in review_list[i].split():\n",
    "        count_lactic+= 1\n",
    "count_tart= 0\n",
    "for i in range(len(review_list)):\n",
    "    if 'tart' in review_list[i].split():\n",
    "        count_tart+= 1\n",
    "IDF_foam  = math.log10(n/(count_foam))\n",
    "IDF_smell  = math.log10(n/(count_smell))\n",
    "IDF_banana  = math.log10(n/(count_banana))\n",
    "IDF_lactic  = math.log10(n/(count_lactic))\n",
    "IDF_tart  = math.log10(n/(count_tart))\n",
    "print('foam',IDF_foam)\n",
    "print('smell',IDF_smell)\n",
    "print('banana',IDF_banana)\n",
    "print('lactic',IDF_lactic)\n",
    "print('tart',IDF_tart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_foam = 0\n",
    "TF_smell = 0\n",
    "TF_banana = 0\n",
    "TF_lactic = 0\n",
    "TF_tart = 0\n",
    "datum = data[0]\n",
    "r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "for w in r.split():\n",
    "    if w =='foam':\n",
    "        TF_foam += 1\n",
    "    if w =='smell':\n",
    "        TF_smell+=1\n",
    "    if w =='banana':\n",
    "        TF_banana+=1\n",
    "    if w =='lactic':\n",
    "        TF_lactic+=1\n",
    "    if w =='tart':\n",
    "        TF_tart+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foam 2.2757372413739256\n",
      "smell 0.5379016188648442\n",
      "banana 3.3555614105321614\n",
      "lactic 5.841637507904751\n",
      "tart 1.8068754016455384\n"
     ]
    }
   ],
   "source": [
    "tf_idf_foam = TF_foam *IDF_foam\n",
    "tf_idf_smell = TF_smell *IDF_smell\n",
    "tf_idf_banana = TF_banana *IDF_banana\n",
    "tf_idf_lactic = TF_lactic *IDF_lactic\n",
    "tf_idf_tart = TF_tart *IDF_tart\n",
    "print('foam',tf_idf_foam)\n",
    "print('smell',tf_idf_smell)\n",
    "print('banana',tf_idf_banana)\n",
    "print('lactic',tf_idf_lactic)\n",
    "print('tart',tf_idf_tart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question4 \n",
    "idf_set =defaultdict(int)\n",
    "count = 0\n",
    "indicator = 0\n",
    "for i in words:\n",
    "    for j in range(len(review_list)):\n",
    "        if i in set(review_list[j].split()):\n",
    "            count+= 1\n",
    "    idf_set[i] = count\n",
    "    indicator+=1\n",
    "    count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in idf_set:\n",
    "    idf_set[i]=math.log10(n/idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_set_r1 = defaultdict(int)\n",
    "tf_set_r2 = defaultdict(int)\n",
    "for i in idf_set:\n",
    "    tf_set_r1[i]=0\n",
    "for i in idf_set:\n",
    "    tf_set_r2[i]=0\n",
    "datum1 = data[0]\n",
    "datum2 = data[1]\n",
    "r1 = ''.join([c for c in datum1['review/text'].lower() if not c in punctuation])\n",
    "r2 = ''.join([c for c in datum2['review/text'].lower() if not c in punctuation])\n",
    "for w in r1.split():\n",
    "    for i in idf_set:\n",
    "        if i == w:\n",
    "            tf_set_r1[i]+=1\n",
    "for w in r2.split():\n",
    "    for i in idf_set:\n",
    "        if i == w:\n",
    "            tf_set_r2[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_r1= {}\n",
    "tf_idf_r2= {}\n",
    "\n",
    "for i in idf_set:\n",
    "    tf_idf_r1[i] = tf_set_r1[i]*idf_set[i]\n",
    "for i in idf_set:\n",
    "    tf_idf_r2[i] = tf_set_r2[i]*idf_set[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10613024167865802"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cosine similarity\n",
    "p= 0\n",
    "k= 0\n",
    "d= 0\n",
    "for i in tf_idf_r1:\n",
    "    p +=tf_idf_r1[i] *tf_idf_r2[i]\n",
    "    k +=tf_idf_r1[i]**2 \n",
    "    d +=tf_idf_r2[i]**2 \n",
    "cos= p/((k**0.5)*(d**0.5))\n",
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    try:\n",
    "        m = np.array(vector1).T\n",
    "        n = np.array(vector2).T\n",
    "        p= sum(m*n)\n",
    "        k = (sum(m*m)**0.5)*(sum(n*n)**0.5)\n",
    "        return p/k\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in range(0,11):\n",
    "    x.append(i*500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "#question 5\n",
    "tf_set_r = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r[i][k]=0\n",
    "    if i in x:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        for j in idf_set:\n",
    "            if j == w:\n",
    "                tf_set_r[i][j]+=1\n",
    "    if i in x:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_r = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r[i] =[]\n",
    "for k in range(len(tf_set_r)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r[k].append(tf_set_r[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cos_dict={}\n",
    "for i in range(len(tf_idf_r)):\n",
    "    cos_dict[i] = cosine_similarity(tf_idf_r[0],tf_idf_r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52211\n",
      "poured from a 22oz bottle to a dogfish head snifter\t\tcolor slight hazy orange with an off white head\t\tsmell cinnamon banana pumpkin and nutmeg\t\ttaste alcohol pumpkin nutmeg allspice and a hint of banana\t\tmouthfeel medium carbonation smooth medium dryness on the palate\t\toverall the smell is great the banana was a huge surprise for me the taste had too much alcohol presence seemed to overpower the other flavors cheers\n"
     ]
    }
   ],
   "source": [
    "cos_dict[0]=0\n",
    "highest_pair = max(cos_dict,key=cos_dict.get)\n",
    "max_data = data[highest_pair]\n",
    "max_text = ''.join([c for c in max_data['review/text'].lower() if not c in punctuation])\n",
    "max_beerID= max_data[ 'beer/beerId']\n",
    "print(max_beerID)\n",
    "print(max_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list=[]\n",
    "new_list = []\n",
    "for i in range(len(tf_idf_r)):\n",
    "    temp_list.append(1)\n",
    "    for j in range(len(tf_idf_r[i])):\n",
    "        temp_list.append(tf_idf_r[i][j])\n",
    "    new_list.append(temp_list)\n",
    "    temp_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 6\n",
    "\n",
    "X = new_list\n",
    "y = [d['review/overall'] for d in data]\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27875956007772235"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= 0\n",
    "for i in range(len(y)):\n",
    "    s +=(y[i]-predictions[i])**2\n",
    "MSE = s/len(y)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question7\n",
    "import random\n",
    "data_final = list(parseData(\"http://jmcauley.ucsd.edu/cse158/data/beer/beer_50000.json\"))[:15000]\n",
    "random.shuffle(data_final)\n",
    "data_train = data_final[0:5000]\n",
    "data_validation = data_final[5000:10000]\n",
    "data_test = data_final[10000:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams-removing-counts situation 1\n",
    "punctuation = set(string.punctuation)\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for l in data_train:\n",
    "    r = ''.join([c for c in l['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "counts1 = [(cobCount[w], w) for w in cobCount]\n",
    "counts1.sort()\n",
    "counts1.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.518359500390911\n",
      "0.1 0.5180331915851174\n",
      "1 0.5149059632746981\n",
      "10 0.493387226900679\n",
      "100 0.48356826924084745\n"
     ]
    }
   ],
   "source": [
    "#validation to get the regularization\n",
    "regularization =[0.01,0.1,1,10,100]\n",
    "pop_bigrams = [x[1] for x in counts1[:1000]]\n",
    "bigramsId = dict(zip(pop_bigrams, range(len(pop_bigrams))))\n",
    "bigramsSet = set(pop_bigrams)\n",
    "def feature(datum):\n",
    "    old_w = ''\n",
    "    cob=''\n",
    "    feat = [0]*len(pop_bigrams)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "        if cob in pop_bigrams:\n",
    "            feat[bigramsId[cob]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data_train]\n",
    "y = [d['review/overall'] for d in data_train]\n",
    "X_validation =[feature(d) for d in data_validation]\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print(k,MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.493292372538536\n"
     ]
    }
   ],
   "source": [
    "    #test,using 100\n",
    "    X = [feature(d) for d in data_train]\n",
    "    y = [d['review/overall'] for d in data_train]\n",
    "    X_test = [feature(d) for d in data_test]\n",
    "    y_test = [d['review/overall'] for d in data_test]\n",
    "\n",
    "    clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print('100',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams-removing-counts situation 2\n",
    "punctuation = set(string.punctuation)\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for l in data_train:\n",
    "    r = ''.join([c for c in l['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "counts2 = [(wordCount[w], w) for w in wordCount]\n",
    "counts2.sort()\n",
    "counts2.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.4529409338050712\n",
      "0.1 0.452668634353386\n",
      "1 0.4500457654993608\n",
      "10 0.4311986124477603\n",
      "100 0.4289422861179684\n"
     ]
    }
   ],
   "source": [
    "#validation to get the regularization\n",
    "regularization =[0.01,0.1,1,10,100]\n",
    "pop_unigrams = [x[1] for x in counts2[:1000]]\n",
    "\n",
    "wordId = dict(zip(pop_unigrams, range(len(pop_unigrams))))\n",
    "wordSet = set(pop_unigrams)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(pop_unigrams)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w in pop_unigrams:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data_train]\n",
    "y = [d['review/overall'] for d in data_train]\n",
    "X_validation =[feature(d) for d in data_validation]\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print(k,MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.42691586891007255\n"
     ]
    }
   ],
   "source": [
    "    #test  k =100\n",
    "    X = [feature(d) for d in data_train]\n",
    "    y = [d['review/overall'] for d in data_train]\n",
    "    X_test = [feature(d) for d in data_test]\n",
    "    y_test = [d['review/overall'] for d in data_test]\n",
    "\n",
    "    clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print('100',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "#unigrams-removing-tfidf situation 3\n",
    "review_list = []\n",
    "data = data_train\n",
    "for datum in data:\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    temporary=''\n",
    "    for w in r.split():\n",
    "        temporary += w +' '\n",
    "    review_list.append(temporary)\n",
    "    \n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts3 = [(wordCount[w], w) for w in wordCount]\n",
    "counts3.sort()\n",
    "counts3.reverse()\n",
    "\n",
    "words = [x[1] for x in counts3[:1000]]\n",
    "\n",
    "x=[100,200,300,400,500,600,700,800,900]\n",
    "\n",
    "\n",
    "idf_set =defaultdict(int)\n",
    "count = 0\n",
    "indicator = 0\n",
    "for i in words:\n",
    "    for j in range(len(review_list)):\n",
    "        if i in set(review_list[j].split()):\n",
    "            count+= 1\n",
    "    idf_set[i] = count\n",
    "    indicator+=1\n",
    "    count=0\n",
    "    if indicator in x:\n",
    "        print(indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "n=5000\n",
    "for i in idf_set:\n",
    "    idf_set[i]=math.log10(n/idf_set[i])\n",
    "tf_set_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_train[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_train[i][k]=0\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_train[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        for j in idf_set:\n",
    "            if j == w:\n",
    "                tf_set_r_train[i][j]+=1\n",
    "tf_idf_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_train[i] =[]\n",
    "for k in range(len(tf_set_r_train)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_train[k].append(tf_set_r_train[k][i]*idf_set[i])\n",
    "tf_set_r_validation = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_validation[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_validation[i][k]=0\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_validation[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        for j in idf_set:\n",
    "            if j == w:\n",
    "                tf_set_r_validation[i][j]+=1\n",
    "tf_idf_r_validation = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_validation[i] =[]\n",
    "for k in range(len(tf_set_r_validation)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_validation[k].append(tf_set_r_validation[k][i]*idf_set[i])\n",
    "tf_set_r_test = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_test[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_test[i][k]=0\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_test[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        for j in idf_set:\n",
    "            if j == w:\n",
    "                tf_set_r_test[i][j]+=1\n",
    "tf_idf_r_test = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_test[i] =[]\n",
    "for k in range(len(tf_set_r_test)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_test[k].append(tf_set_r_test[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_train=[]\n",
    "new_list_train = []\n",
    "for i in range(len(tf_idf_r_train)):\n",
    "    temp_list_train.append(1)\n",
    "    for j in range(len(tf_idf_r_train[i])):\n",
    "        temp_list_train.append(tf_idf_r_train[i][j])\n",
    "    new_list_train.append(temp_list_train)\n",
    "    temp_list_train=[]\n",
    "X_train = new_list_train\n",
    "y_train = [d['review/overall'] for d in data_train]\n",
    "\n",
    "temp_list_validation=[]\n",
    "new_list_validation = []\n",
    "for i in range(len(tf_idf_r_validation)):\n",
    "    temp_list_validation.append(1)\n",
    "    for j in range(len(tf_idf_r_validation[i])):\n",
    "        temp_list_validation.append(tf_idf_r_validation[i][j])\n",
    "    new_list_validation.append(temp_list_validation)\n",
    "    temp_list_validation=[]\n",
    "X_validation = new_list_validation\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "\n",
    "temp_list_test=[]\n",
    "new_list_test = []\n",
    "for i in range(len(tf_idf_r_test)):\n",
    "    temp_list_test.append(1)\n",
    "    for j in range(len(tf_idf_r_test[i])):\n",
    "        temp_list_test.append(tf_idf_r_test[i][j])\n",
    "    new_list_test.append(temp_list_test)\n",
    "    temp_list_test=[]\n",
    "X_test = new_list_test\n",
    "y_test = [d['review/overall'] for d in data_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.42938963546198694\n",
      "0.1 0.4293010105047813\n",
      "1 0.42849054310134843\n",
      "10 0.42249042777884943\n",
      "100 0.4336330459164157\n"
     ]
    }
   ],
   "source": [
    "regularization =[0.01,0.1,1,10,100]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y_validation)\n",
    "    print(k,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.4461932505297838\n"
     ]
    }
   ],
   "source": [
    "    #best k is 10\n",
    "    clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y_test)\n",
    "    print(10,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biograms-removing-tfidf situation 4\n",
    "\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "\n",
    "counts4 = [(cobCount[w], w) for w in cobCount]\n",
    "counts4.sort()\n",
    "counts4.reverse()\n",
    "\n",
    "words = [x[1] for x in counts4[:1000]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_set =defaultdict(int)\n",
    "count = 0\n",
    "indicator = 0\n",
    "x =[10,20,50,100,200,300,400,500,600,700,800,900]\n",
    "for i in words:\n",
    "    for j in range(len(review_list)):\n",
    "        old_w = ''\n",
    "        cob=''\n",
    "        cobCount = []\n",
    "        for w in review_list[j].split():\n",
    "            if old_w != '':\n",
    "                cob =old_w + ' '+ w\n",
    "                judgement = True\n",
    "            else :\n",
    "                cob =''\n",
    "                judgement = False\n",
    "            if judgement == True:\n",
    "                cobCount.append(cob)    \n",
    "                old_w = w\n",
    "            else:\n",
    "                old_w = w\n",
    "        for k in cobCount:\n",
    "            if k == i:\n",
    "                idf_set[i]+= 1\n",
    "for i in idf_set:\n",
    "idf_set[i]=math.log10(n/(idf_set[i]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_set_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_train[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_train[i][k]=0\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_train[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "            old_w = w\n",
    "        else:\n",
    "            cob =''  \n",
    "            old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_train[i][j]+=1\n",
    "tf_idf_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_train[i] =[]\n",
    "for k in range(len(tf_set_r_train)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_train[k].append(tf_set_r_train[k][i]*idf_set[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_set_r_validation = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_validation[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_validation[i][k]=0\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_validation[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "            old_w = w\n",
    "        else:\n",
    "            cob =''  \n",
    "            old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_validation[i][j]+=1\n",
    "tf_idf_r_validation = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_validation[i] =[]\n",
    "for k in range(len(tf_set_r_validation)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_validation[k].append(tf_set_r_validation[k][i]*idf_set[i])\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "tf_set_r_test = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_test[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_test[i][k]=0\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_test[i]['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "            old_w = w\n",
    "        else:\n",
    "            cob =''  \n",
    "            old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_test[i][j]+=1\n",
    "tf_idf_r_test = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_test[i] =[]\n",
    "for k in range(len(tf_set_r_test)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_test[k].append(tf_set_r_test[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_train=[]\n",
    "new_list_train = []\n",
    "for i in range(len(tf_idf_r_train)):\n",
    "    temp_list_train.append(1)\n",
    "    for j in range(len(tf_idf_r_train[i])):\n",
    "        temp_list_train.append(tf_idf_r_train[i][j])\n",
    "    new_list_train.append(temp_list_train)\n",
    "    temp_list_train=[]\n",
    "X_train = new_list_train\n",
    "y_train = [d['review/overall'] for d in data_train]\n",
    "\n",
    "temp_list_validation=[]\n",
    "new_list_validation = []\n",
    "for i in range(len(tf_idf_r_validation)):\n",
    "    temp_list_validation.append(1)\n",
    "    for j in range(len(tf_idf_r_validation[i])):\n",
    "        temp_list_validation.append(tf_idf_r_validation[i][j])\n",
    "    new_list_validation.append(temp_list_validation)\n",
    "    temp_list_validation=[]\n",
    "X_validation = new_list_validation\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "\n",
    "temp_list_test=[]\n",
    "new_list_test = []\n",
    "for i in range(len(tf_idf_r_test)):\n",
    "    temp_list_test.append(1)\n",
    "    for j in range(len(tf_idf_r_test[i])):\n",
    "        temp_list_test.append(tf_idf_r_test[i][j])\n",
    "    new_list_test.append(temp_list_test)\n",
    "    temp_list_test=[]\n",
    "X_test = new_list_test\n",
    "y_test = [d['review/overall'] for d in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.5016421359342702\n",
      "0.1 0.5015109865034666\n",
      "1 0.5005300661904458\n",
      "10 0.49339567675744767\n",
      "100 0.49246433182787\n"
     ]
    }
   ],
   "source": [
    "regularization =[0.01,0.1,1,10,100]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y_validation)\n",
    "    print(k,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.5195463603282625\n"
     ]
    }
   ],
   "source": [
    "    #best k is 100\n",
    "    clf = linear_model.Ridge(100, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y_test)\n",
    "    print(100,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biograms-not removing-tfidf situation 5\n",
    "data = data_train\n",
    "import nltk\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower()]) \n",
    "    for w in nltk.word_tokenize(r):\n",
    "        cob =old_w + ' '+ w\n",
    "        judgement = True\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "\n",
    "counts5 = [(cobCount[w], w) for w in cobCount]\n",
    "counts5.sort()\n",
    "counts5.reverse()\n",
    "\n",
    "words = [x[1] for x in counts5[:1000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-936885d94dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcobCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mcob\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mold_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mjudgement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idf_set =defaultdict(int)\n",
    "count = 0\n",
    "indicator = 0\n",
    "x =[10,20,50,100,200,300,400,500,600,700,800,900]\n",
    "for i in words:\n",
    "    for j in range(len(review_list)):\n",
    "        old_w = ''\n",
    "        cob=''\n",
    "        cobCount = []\n",
    "        for w in nltk.word_tokenize(review_list[j]):\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "            if judgement == True:\n",
    "                cobCount.append(cob)    \n",
    "                old_w = w\n",
    "            else:\n",
    "                old_w = w\n",
    "        for k in cobCount:\n",
    "            if k == i:\n",
    "                idf_set[i]+= 1\n",
    "for i in idf_set:\n",
    "    idf_set[i]=math.log10(n/(idf_set[i]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_set_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_train[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_train[i][k]=0\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_train[i]['review/text'].lower() ])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        cob =old_w + ' '+ w\n",
    "        judgement = True\n",
    "        old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_train[i][j]+=1\n",
    "tf_idf_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_train[i] =[]\n",
    "for k in range(len(tf_set_r_train)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_train[k].append(tf_set_r_train[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_set_r_validation = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_validation[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_validation[i][k]=0\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_validation[i]['review/text'].lower() ])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        cob =old_w + ' '+ w\n",
    "        judgement = True\n",
    "        old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_validation[i][j]+=1\n",
    "tf_idf_r_validation = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_validation[i] =[]\n",
    "for k in range(len(tf_set_r_validation)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_validation[k].append(tf_set_r_validation[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_set_r_test = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_test[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_test[i][k]=0\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_test[i]['review/text'].lower() ])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        cob =old_w + ' '+ w\n",
    "        judgement = True\n",
    "        old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_test[i][j]+=1\n",
    "tf_idf_r_test = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_test[i] =[]\n",
    "for k in range(len(tf_set_r_test)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_test[k].append(tf_set_r_test[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_train=[]\n",
    "new_list_train = []\n",
    "for i in range(len(tf_idf_r_train)):\n",
    "    temp_list_train.append(1)\n",
    "    for j in range(len(tf_idf_r_train[i])):\n",
    "        temp_list_train.append(tf_idf_r_train[i][j])\n",
    "    new_list_train.append(temp_list_train)\n",
    "    temp_list_train=[]\n",
    "X_train = new_list_train\n",
    "y_train = [d['review/overall'] for d in data_train]\n",
    "\n",
    "temp_list_validation=[]\n",
    "new_list_validation = []\n",
    "for i in range(len(tf_idf_r_validation)):\n",
    "    temp_list_validation.append(1)\n",
    "    for j in range(len(tf_idf_r_validation[i])):\n",
    "        temp_list_validation.append(tf_idf_r_validation[i][j])\n",
    "    new_list_validation.append(temp_list_validation)\n",
    "    temp_list_validation=[]\n",
    "X_validation = new_list_validation\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "\n",
    "temp_list_test=[]\n",
    "new_list_test = []\n",
    "for i in range(len(tf_idf_r_test)):\n",
    "    temp_list_test.append(1)\n",
    "    for j in range(len(tf_idf_r_test[i])):\n",
    "        temp_list_test.append(tf_idf_r_test[i][j])\n",
    "    new_list_test.append(temp_list_test)\n",
    "    temp_list_test=[]\n",
    "X_test = new_list_test\n",
    "y_test = [d['review/overall'] for d in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.5029736634134457\n",
      "0.1 0.5029452188735911\n",
      "1 0.5026438855207455\n",
      "10 0.49963811353783966\n",
      "100 0.5088605762376635\n"
     ]
    }
   ],
   "source": [
    "regularization =[0.01,0.1,1,10,100]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y_validation)\n",
    "    print(k,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.49037692103212255\n"
     ]
    }
   ],
   "source": [
    "    #best k is 10\n",
    "    clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y_test)\n",
    "    print(10,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniograms-not removing-tfidf situation 6\n",
    "review_list = []\n",
    "data = data_train\n",
    "\n",
    "for datum in data:\n",
    "    r = ''.join([c for c in datum['review/text'].lower() ])\n",
    "    temporary=''\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        temporary += w +' '\n",
    "    review_list.append(temporary)\n",
    "    \n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() ])\n",
    "  for w in nltk.word_tokenize(r):\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts6 = [(wordCount[w], w) for w in wordCount]\n",
    "counts6.sort()\n",
    "counts6.reverse()\n",
    "\n",
    "words = [x[1] for x in counts6[:1000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-23b80d0a5d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcobCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mcob\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mold_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mjudgement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m   1321\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \"\"\"\n\u001b[1;32m   1342\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \"\"\"\n\u001b[0;32m-> 1478\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \"\"\"\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_first_pass_annotation\u001b[0;34m(self, aug_tok)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \"\"\"\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_end_chars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#It is too slow to run oh my god\n",
    "idf_set =defaultdict(int)\n",
    "count = 0\n",
    "indicator = 0\n",
    "for i in words:\n",
    "    for j in range(len(review_list)):\n",
    "        old_w = ''\n",
    "        cob=''\n",
    "        cobCount = []\n",
    "        for w in nltk.word_tokenize(review_list[j]):\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "            if judgement == True:\n",
    "                cobCount.append(cob)    \n",
    "                old_w = w\n",
    "            else:\n",
    "                old_w = w\n",
    "        for k in cobCount:\n",
    "            if k == i:\n",
    "                idf_set[i]+= 1\n",
    "for i in idf_set:\n",
    "    idf_set[i]=math.log10(n/(idf_set[i]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_set_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_set_r_train[i]=defaultdict(int)\n",
    "    for k in idf_set:\n",
    "        tf_set_r_train[i][k]=0\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data_train[i]['review/text'].lower() ])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        cob =old_w + ' '+ w\n",
    "        judgement = True\n",
    "        old_w = w\n",
    "        for j in idf_set:\n",
    "            if j == cob:\n",
    "                tf_set_r_train[i][j]+=1\n",
    "tf_idf_r_train = defaultdict(int)\n",
    "for i in range(0,5000):\n",
    "    tf_idf_r_train[i] =[]\n",
    "for k in range(len(tf_set_r_train)):\n",
    "    for i in idf_set:\n",
    "        tf_idf_r_train[k].append(tf_set_r_train[k][i]*idf_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biograms-not removing-counts situation 7\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for l in data_train:\n",
    "    r = ''.join([c for c in l['review/text'].lower()])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "counts7 = [(cobCount[w], w) for w in cobCount]\n",
    "counts7.sort()\n",
    "counts7.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.5222570758086934\n",
      "0.1 0.5220068594032546\n",
      "1 0.5196136444777321\n",
      "10 0.5027507215657296\n",
      "100 0.4978533182387553\n"
     ]
    }
   ],
   "source": [
    "regularization =[0.01,0.1,1,10,100]\n",
    "pop_bigrams = [x[1] for x in counts7[:1000]]\n",
    "bigramsId = dict(zip(pop_bigrams, range(len(pop_bigrams))))\n",
    "bigramsSet = set(pop_bigrams)\n",
    "def feature(datum):\n",
    "    old_w = ''\n",
    "    cob=''\n",
    "    feat = [0]*len(pop_bigrams)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() ])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        if old_w != '':\n",
    "            cob =old_w + ' '+ w\n",
    "            judgement = True\n",
    "        else:\n",
    "            cob =''\n",
    "            judgement = False\n",
    "        if judgement == True:\n",
    "            cobCount[cob] += 1    \n",
    "            old_w = w\n",
    "        else:\n",
    "            old_w = w\n",
    "        if cob in pop_bigrams:\n",
    "            feat[bigramsId[cob]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data_train]\n",
    "y = [d['review/overall'] for d in data_train]\n",
    "X_validation =[feature(d) for d in data_validation]\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print(k,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.48003136101779\n"
     ]
    }
   ],
   "source": [
    "    #test,using 100\n",
    "    X = [feature(d) for d in data_train]\n",
    "    y = [d['review/overall'] for d in data_train]\n",
    "    X_test = [feature(d) for d in data_test]\n",
    "    y_test = [d['review/overall'] for d in data_test]\n",
    "\n",
    "    clf = linear_model.Ridge(100, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print('100',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.43760267423644056\n",
      "0.1 0.43735558819960496\n",
      "1 0.4350170523269765\n",
      "10 0.4199182162315345\n",
      "100 0.43845429656950863\n"
     ]
    }
   ],
   "source": [
    "#uiograms-not removing-counts situation 8\n",
    "old_w = ''\n",
    "cob=''\n",
    "cobCount = defaultdict(int)\n",
    "for l in data_train:\n",
    "    r = ''.join([c for c in l['review/text'].lower() ])\n",
    "    for w in nltk.word_tokenize(r):\n",
    "        wordCount[w] += 1\n",
    "counts8 = [(wordCount[w], w) for w in wordCount]\n",
    "counts8.sort()\n",
    "counts8.reverse()\n",
    "regularization =[0.01,0.1,1,10,100]\n",
    "pop_unigrams = [x[1] for x in counts8[:1000]]\n",
    "\n",
    "wordId = dict(zip(pop_unigrams, range(len(pop_unigrams))))\n",
    "wordSet = set(pop_unigrams)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(pop_unigrams)\n",
    "  r = ''.join([c for c in datum['review/text'].lower() ])\n",
    "  for w in nltk.word_tokenize(r):\n",
    "    if w in pop_unigrams:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data_train]\n",
    "y = [d['review/overall'] for d in data_train]\n",
    "X_validation =[feature(d) for d in data_validation]\n",
    "y_validation = [d['review/overall'] for d in data_validation]\n",
    "for k in regularization : \n",
    "    clf = linear_model.Ridge(k, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_validation)\n",
    "    s= 0\n",
    "    for i in range(len(y_validation)):\n",
    "        s +=(y_validation[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print(k,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.41616258466156125\n"
     ]
    }
   ],
   "source": [
    "    #test,using 10\n",
    "    X = [feature(d) for d in data_train]\n",
    "    y = [d['review/overall'] for d in data_train]\n",
    "    X_test = [feature(d) for d in data_test]\n",
    "    y_test = [d['review/overall'] for d in data_test]\n",
    "\n",
    "    clf = linear_model.Ridge(10, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(X_test)\n",
    "    s= 0\n",
    "    for i in range(len(y_test)):\n",
    "        s +=(y_test[i]-predictions[i])**2\n",
    "    MSE = s/len(y)\n",
    "    print('10',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so the matrix is \n",
    "#bigrams-removing-counts situation 1     regulization is 100 MSE 0.4932\n",
    "#unigrams-removing-counts situation 2     regulization is 100 MSE 0.4269\n",
    "#unigrams-removing-tfidf situation 3      regulization is 10   MSE 0.4461\n",
    "#biograms-removing-tfidf situation 4      regulization  is 10   MSE 0.519\n",
    "#biograms-not removing-tfidf situation 5   regulization is  10  MSE 0.49037\n",
    "#uniograms-not removing-tfidf situation 6  too slow to get the result\n",
    "#biograms-not removing-counts situation 7    regulization is 100  MSE is 0.4800\n",
    "#uiograms-not removing-counts situation 8    regulization is 10  MSE is 0.4161626"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
