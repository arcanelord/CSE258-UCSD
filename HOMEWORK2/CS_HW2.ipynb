{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "Collecting decorator>=4.3.0 (from networkx)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/bb/a24838832ba35baf52f32ab1a49b906b5f82fb7c76b2f6a7e35e140bac30/decorator-4.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: decorator, networkx\n",
      "Successfully installed decorator-4.3.0 networkx-2.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user networkx\n",
    "import numpy\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "import ast\n",
    "from math import exp\n",
    "from math import log\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "## import data\n",
    "def parseDataFromFile(fname):\n",
    "  for l in open(\"beer_50000.json\").read().splitlines():\n",
    "    yield ast.literal_eval(l)\n",
    "print (\"Reading data...\")\n",
    "data = list(parseDataFromFile(\"beer_50000.json\"))\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random data\n",
    "random.shuffle(data)\n",
    "\n",
    "## split\n",
    "training_data= data[:int(len(data)/3)]\n",
    "vali_data= data[int(len(data)/3):2*int(len(data)/3)]\n",
    "test_data= data[2*int(len(data)/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], datum['review/palate'], datum['review/overall']]\n",
    "  return feat\n",
    "\n",
    "X_train = [feature(d) for d in training_data]\n",
    "y_train = [d['beer/ABV'] >= 6.5 for d in training_data]\n",
    "\n",
    "X_vali = [feature(d) for d in vali_data]\n",
    "y_vali = [d['beer/ABV'] >= 6.5 for d in vali_data]\n",
    "\n",
    "X_test = [feature(d) for d in test_data]\n",
    "y_test = [d['beer/ABV'] >= 6.5 for d in test_data]\n",
    "\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return numpy.array([-x for x in dl])\n",
    "\n",
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X_train[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n",
    "\n",
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta, X, y):\n",
    "  scores = [inner(theta,x) for x in X]\n",
    "  predictions = [s > 0 for s in scores]\n",
    "  correct = [(a==b) for (a,b) in zip(predictions,y)]\n",
    "  acc = sum(correct) * 1.0 / len(correct)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1.0:\taccuracy of train set=0.7205688227529101\n",
      "lambda = 1.0:\taccuracy of validation set=0.7206288251530061\n",
      "lambda = 1.0:\taccuracy of test set=0.7150227981761459\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "theta = train(lam)\n",
    "acc = performance(theta, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of train set=\" + str(acc))\n",
    "\n",
    "acc = performance(theta, X_vali, y_vali)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of validation set=\" + str(acc))\n",
    "\n",
    "acc = performance(theta, X_test, y_test)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of test set=\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_perf(theta, X, y):\n",
    "    scores = [inner(theta,x) for x in X]\n",
    "    predictions = [s > 0 for s in scores] # output TF\n",
    "    correct = [(a==b) for (a,b) in zip(predictions,y)]\n",
    "    P = predictions.count(True)\n",
    "    N = predictions.count(False)\n",
    "    TP = [(a*b) for (a,b) in zip(predictions,correct)]\n",
    "    TP = TP.count(1)\n",
    "    TN = [ ((1-a)*b) for (a,b) in zip(predictions,correct)]\n",
    "    TN = TN.count(1)\n",
    "    FP = abs(P- TP)\n",
    "    FN = abs(N- TN)\n",
    "    result = [P,N,TP,TN,FP,FN]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive = 12368\tNegative=4300\tTruePositive=8963\tTrueNegative=2955\tFalsePositive=3405\tFalseNegative=1345\n"
     ]
    }
   ],
   "source": [
    "result_test = det_perf(theta, X_test, y_test)\n",
    "print(\"Positive = \" + str(result_test[0]) + \"\\tNegative=\" + str(result_test[1]) + \"\\tTruePositive=\" + str(result_test[2]) + \n",
    "      \"\\tTrueNegative=\" + str(result_test[3]) + \"\\tFalsePositive=\" + str(result_test[4]) + \"\\tFalseNegative=\" + str(result_test[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to assign greater importance to False Positives, then I will change the coefficient.\n",
    "\n",
    "Firstly, I will change the f function's for parts. I will devide them into four parts. Now they are devided into two parts   $$y[i] > 0$$ $$y[i] < 0$$\n",
    "\n",
    "I will add two judgment. So they will have four situation:$$y[i] > 0\\quad and\\quad X[i] \\cdot \\theta > 0$$\n",
    "$$y[i] > 0\\quad and\\quad X[i] \\cdot \\theta < 0$$\n",
    "$$y[i] < 0\\quad and\\quad X[i] \\cdot \\theta > 0$$\n",
    "$$y[i] < 0\\quad and\\quad X[i] \\cdot \\theta < 0$$\n",
    "\n",
    "Secondly, I will assign 10 times loglikelihood of the situation $y[i] < 0\\quad and\\quad X[i] \\cdot \\theta > 0$, which means False Positive.\n",
    "\n",
    "Thirdly, I will change the Derivative of log-likelihood correspondently. Add two more judgments and change the coefficient of the False Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0:\taccuracy of train set=0.7217088683547341\n",
      "lambda = 0:\taccuracy of validation set=0.7209288371534861\n"
     ]
    }
   ],
   "source": [
    "## lambda = 0\n",
    "lam = 0\n",
    "\n",
    "theta1 = train(lam)\n",
    "acc = performance(theta1, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of train set=\" + str(acc))\n",
    "\n",
    "acc1 = performance(theta1, X_vali, y_vali)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of validation set=\" + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.01:\taccuracy of train set=0.7217088683547341\n",
      "lambda = 0.01:\taccuracy of validation set=0.7209288371534861\n"
     ]
    }
   ],
   "source": [
    "## lambda = 0.01\n",
    "lam = 0.01\n",
    "\n",
    "theta2 = train(lam)\n",
    "acc = performance(theta2, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of train set=\" + str(acc))\n",
    "\n",
    "acc2 = performance(theta2, X_vali, y_vali)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of validation set=\" + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.1:\taccuracy of train set=0.7215288611544461\n",
      "lambda = 0.1:\taccuracy of validation set=0.7210488419536781\n"
     ]
    }
   ],
   "source": [
    "## lambda = 0.1\n",
    "lam = 0.1\n",
    "\n",
    "theta3 = train(lam)\n",
    "acc = performance(theta3, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of train set=\" + str(acc))\n",
    "\n",
    "acc3 = performance(theta3, X_vali, y_vali)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of validation set=\" + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1:\taccuracy of train set=0.7205688227529101\n",
      "lambda = 1:\taccuracy of validation set=0.7206288251530061\n"
     ]
    }
   ],
   "source": [
    "## lambda = 1\n",
    "lam = 1\n",
    "\n",
    "theta4 = train(lam)\n",
    "acc = performance(theta4, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of train set=\" + str(acc))\n",
    "\n",
    "acc4 = performance(theta4, X_vali, y_vali)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of validation set=\" + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 100:\taccuracy of train set=0.6728669146765871\n",
      "lambda = 100:\taccuracy of validation set=0.6689067562702508\n"
     ]
    }
   ],
   "source": [
    "## lambda = 100\n",
    "lam = 100\n",
    "\n",
    "theta5 = train(lam)\n",
    "acc = performance(theta5, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of train set=\" + str(acc))\n",
    "\n",
    "acc5 = performance(theta5, X_vali, y_vali)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of validation set=\" + str(acc5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the accracy shown above:\n",
    "\n",
    "the theta with best accracy is $\\theta_4$, when $\\lambda$ equals 1.\n",
    "    \n",
    "For the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1:\taccuracy of test set=0.7150227981761459\n"
     ]
    }
   ],
   "source": [
    "lam = 1\n",
    "acc = performance(theta4, X_test, y_test)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy of test set=\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('egonet.txt', 'r') as f2:\n",
    "    data = f2.read()\n",
    "ego_list = [x for x in data.split('\\n')]\n",
    "ego_list.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = set()\n",
    "nodes = set()\n",
    "a = list()\n",
    "for edge in ego_list:\n",
    "  x,y = edge.split(' ')\n",
    "  x,y = int(x),int(y)\n",
    "  edges.add((x,y))\n",
    "  edges.add((y,x))\n",
    "  nodes.add(x)\n",
    "  nodes.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get the number of connected components\n",
    "num_components = nx.number_connected_components(G)\n",
    "print(num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# Get the number of nodes in the largest connected component\n",
    "G_large = max(nx.connected_components(G), key = len)  ####question???\n",
    "num_nodes_large = len(G_large)\n",
    "print(num_nodes_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "GL= max(nx.connected_component_subgraphs(G), key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_set = list(GL.nodes())\n",
    "G_set.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=list()\n",
    "c2=list()\n",
    "for i in range(int(len(G_set))):\n",
    "    if i < int(len(G_set)/2):\n",
    "        c1.append(G_set[i])\n",
    "    else:\n",
    "        c2.append(G_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cut = (nx.cut_size(GL, c1, c2)/sum([GL.degree(v) for v in c1])+nx.cut_size(GL, c1, c2)/sum([GL.degree(v) for v in c2]))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42240587695133147"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 804, 805, 810, 811, 819]\n",
      "[823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893, 729]\n",
      "[697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819]\n",
      "[823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893, 729, 804]\n",
      "[697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819, 729]\n",
      "[823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893, 804]\n",
      "[697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819]\n",
      "[823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893, 804, 729]\n",
      "We need to Iterate for 3 times and our best normalized cut cost is  0.3068340306834031\n"
     ]
    }
   ],
   "source": [
    "## initial\n",
    "c1=list()\n",
    "c2=list()\n",
    "for i in range(int(len(G_set))):\n",
    "    if i < int(len(G_set)/2):\n",
    "        c1.append(G_set[i])\n",
    "    else:\n",
    "        c2.append(G_set[i])\n",
    "        \n",
    "def N_cut(c1,c2):\n",
    "    N = (nx.cut_size(GL, c1, c2)/sum([GL.degree(v) for v in c1])+nx.cut_size(GL, c1, c2)/sum([GL.degree(v) for v in c2]))/2\n",
    "    return N\n",
    "\n",
    "\n",
    "## greedy \n",
    "\n",
    "norcut=list([1,0.9])\n",
    "n = -1\n",
    "while norcut[-2] - norcut[-1] >= 0:\n",
    "    c3=list()\n",
    "    \n",
    "    for i in range(len(c1)):\n",
    "        c4, c5= c1, c2\n",
    "        com = c4[i]\n",
    "        c5.append(c4[i])\n",
    "        del c4[i]\n",
    "        c3.append(N_cut(c4,c5))\n",
    "        c1.insert(i,com)\n",
    "        c2.pop()\n",
    "    for i in range(len(c2)):\n",
    "        c4, c5= c1, c2\n",
    "        com = c5[i]\n",
    "        c4.append(c5[i])\n",
    "        del c5[i]\n",
    "        c3.append(N_cut(c4,c5))\n",
    "        c2.insert(i,com)\n",
    "        c1.pop()\n",
    "    norcut.append(min(c3))\n",
    "    n= n+1\n",
    "    \n",
    "    if c3.index(min(c3)) <= 19:\n",
    "        c2.append(c1[c3.index(min(c3))])\n",
    "        del c1[c3.index(min(c3))]\n",
    "        print(c1)\n",
    "        print(c2)\n",
    "    else:\n",
    "        c1.append(c2[c3.index(min(c3))])\n",
    "        del c2[c3.index(min(c3))]\n",
    "        print(c1)\n",
    "        print(c2)\n",
    "print('We need to Iterate for '+str(n)+' times and our best normalized cut cost is  ' + str(norcut[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the best way to split this is \n",
    "\n",
    "[697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819, 729]\n",
    "\n",
    "[823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893, 804]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norcut[-2] - norcut[-1] >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c9 =[825,861,863,864,876,878,882,884,886,888,889,893,729,804]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c8 =[697,703,708,713,719,745,747,769,772,774,798,800,805,810,811,819,828,830,823,840,880,803,869,890,753,856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "c10 = c8.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[697,\n",
       " 703,\n",
       " 708,\n",
       " 713,\n",
       " 719,\n",
       " 745,\n",
       " 747,\n",
       " 769,\n",
       " 772,\n",
       " 774,\n",
       " 798,\n",
       " 800,\n",
       " 805,\n",
       " 810,\n",
       " 811,\n",
       " 819,\n",
       " 828,\n",
       " 830,\n",
       " 823,\n",
       " 840,\n",
       " 880,\n",
       " 803,\n",
       " 869,\n",
       " 890,\n",
       " 753,\n",
       " 856]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "del c10[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
